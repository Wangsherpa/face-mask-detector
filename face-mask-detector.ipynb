{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19: Face Mask Detector with OpenCV, Pytorch and Deep Learning\n",
    "\n",
    "In order to train a custom face mask detector, we need to break our project into two distinct phases, each with its own respective sub-steps\n",
    "1. **Training**: Here we'll focus on loading our face mask detection dataset from disk, training a model (using Pytorch) on this dataset, and then serializing the face mas detector to disk.\n",
    "2. **Deployment**: Once the face mask detector is trained, we can then move on to loading the mask detector, performing face detection, and then classifying each face as with_mask or without_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import imutils\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn, optim\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import models, datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset from the dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Training Images: 1176, No of Testing Images: 200\n"
     ]
    }
   ],
   "source": [
    "# define data directory\n",
    "data_dir = 'dataset/'\n",
    "\n",
    "# define train, test directory\n",
    "train_dir = os.path.join(data_dir, 'train/')\n",
    "test_dir = os.path.join(data_dir, 'test/')\n",
    "\n",
    "# define transformations\n",
    "transform = transforms.Compose([transforms.Resize(256),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                     [0.229, 0.224, 0.225])\n",
    "                               ])\n",
    "\n",
    "# load train and test data\n",
    "train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "# define dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=10,\n",
    "                                           shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=10,\n",
    "                                          shuffle = True)\n",
    "\n",
    "# Print the number of pictures in the datasets\n",
    "print(f\"No of Training Images: {len(train_data)},\",\n",
    "      f\"No of Testing Images: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the vgg16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "model = models.vgg16(pretrained=True)\n",
    "\n",
    "# freeze the weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Check if gpu is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# no of parameters comming from last cnn layer\n",
    "n_inputs = model.classifier[6].in_features\n",
    "\n",
    "# Change the last layer according to our need\n",
    "model.classifier[6] = nn.Linear(n_inputs, 2)\n",
    "\n",
    "# if gpu is available then add model to gpu\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# verify if the model architecture is as expected\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 15\n",
    "\n",
    "def train(model):\n",
    "    for e in range(n_epochs):\n",
    "        train_loss = 0\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            if use_cuda:\n",
    "                images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "            # Clear previous accumulated gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            output = model(images)\n",
    "\n",
    "            # calculate loss\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            # backward pass (backpropagation)\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update train_loss\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        torch.save(model.state_dict(), 'model-1-acc-99.pt')\n",
    "        print(f\" Epochs: {e+1}/{n_epochs}, Train_loss : {train_loss / len(train_loader)}\")\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e75b5b6ed184d1e91ffd6f0e357e450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 0.021583\n",
      "\n",
      "\n",
      " Test Accuracy: 99% (199/200)\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0.\n",
    "correct = 0.\n",
    "total = 0.\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for batch_idx, (data, target) in tqdm(enumerate(test_loader)):\n",
    "    \n",
    "    if use_cuda:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    \n",
    "    # forward pass\n",
    "    output = model(data)\n",
    "    \n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    \n",
    "    # update the averae test loss\n",
    "    test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "    \n",
    "    # convert the output probabilities to predicted class\n",
    "    pred = output.data.max(1, keepdim=True)[1]\n",
    "    \n",
    "    # compare the prediction to true label\n",
    "    correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "    total += data.size(0)\n",
    "    \n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "print('\\n Test Accuracy: %2d%% (%2d/%2d)' % (100. * correct / total, correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test custom images\n",
    "from PIL import Image\n",
    "\n",
    "def with_or_without_mask(image, model):\n",
    "    input_img = Image.fromarray(image)\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    input_tensor = preprocess(input_img)\n",
    "    input_batch = input_tensor.unsqueeze(0)\n",
    "    \n",
    "    # Move the input to gpu if available\n",
    "    if use_cuda:\n",
    "        input_batch = input_batch.cuda()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "        \n",
    "    prob, index = torch.max(F.softmax(output[0]), 0)\n",
    "    \n",
    "    # predicted class index and probability\n",
    "    return prob, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'with_mask': 0, 'without_mask': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.class_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detector from Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading face detector model...\n",
      "Loading face mask detector model\n",
      "Computing face detections...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda\\envs\\deep_learning\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# loading our serialized face detector model from disk\n",
    "print(\"Loading face detector model...\")\n",
    "prototxtPath = 'face_detector/deploy.prototxt'\n",
    "weightsPath = 'face_detector/res10_300x300_ssd_iter_140000.caffemodel'\n",
    "\n",
    "net = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
    "\n",
    "# load the face mask detector model from disk\n",
    "print(\"Loading face mask detector model\")\n",
    "model.load_state_dict(torch.load('model-1-acc-99.pt'))\n",
    "\n",
    "# load the input image from disk, clone it, and grab the image dimensions\n",
    "image = cv2.imread('examples/multiple_face.jpg')\n",
    "image = imutils.resize(image, width=400)\n",
    "# cap = cv2.VideoCapture('http://192.168.225.24:8080/video')\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# _, image = cap.read()\n",
    "orig = image.copy()\n",
    "(h, w) = image.shape[:2]\n",
    "\n",
    "# construct a blob from the image\n",
    "blob = cv2.dnn.blobFromImage(image, 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "\n",
    "# pass the blob through the network and obtain the face detections\n",
    "print(\"Computing face detections...\")\n",
    "net.setInput(blob)\n",
    "detections = net.forward()\n",
    "\n",
    "# loop over the detections\n",
    "for i in range(0, detections.shape[2]):\n",
    "    # extract the confidence (i.e, probability) associated with the detection\n",
    "    confidence = detections[0, 0, i, 2]\n",
    "    \n",
    "    # filter out weak detections by ensuring the confidence is greater\n",
    "    # than the minimum confidence\n",
    "    if confidence > 0.5:\n",
    "        # compute the (x, y)-coordinates of the bounding box for the object\n",
    "        box = detections[0, 0, i, 3:7]  * np.array([w, h, w, h])\n",
    "        (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "        \n",
    "        # ensure the bounding boxes fall within the dimensions of the frame\n",
    "        (startX, startY) = (max(0, startX), max(0, startY))\n",
    "        (endX, endY) = (min(w-1, endX), min(h-1, endY))\n",
    "        \n",
    "        # preprocess the image and get the class and proba\n",
    "        prob, class_ = with_or_without_mask(image[startY:endY, startX:endX], model)\n",
    "        \n",
    "        # determine the class label and color we'll use to draw the bounding\n",
    "        # box and text\n",
    "        label = \"Mask\" if class_ == 0 else \"No Mask\"\n",
    "        color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
    "        \n",
    "        # include the probability in the label\n",
    "        label = \"{}: {:.2f}%\".format(label, prob*100)\n",
    "        \n",
    "        # display the label and bounding box rectangle on the output frame\n",
    "        cv2.putText(image, label, (startX, startY - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "        cv2.rectangle(image, (startX, startY), (endX, endY), color, 2)\n",
    "\n",
    "# show the output image\n",
    "cv2.imshow(\"output\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detector in a real time Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_predict(frame, faceNet, maskNet):\n",
    "    # grab the dimension of the frame and then construct a blob from it\n",
    "    (h, w) = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300),\n",
    "                                 (104.0, 177.0, 123.0))\n",
    "    \n",
    "    # pass the blob through the network and obtain the face detections\n",
    "    faceNet.setInput(blob)\n",
    "    detections = faceNet.forward()\n",
    "    \n",
    "    # initialize our list of faces, their corresponding locations\n",
    "    # and the list of predictions from our face mask network\n",
    "    faces = []\n",
    "    locs = []\n",
    "    preds = []\n",
    "    \n",
    "    # loop over the detections\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        # extract the confidence associated with the detections\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        \n",
    "        # filter out weak detections by ensuring the confidence greater than\n",
    "        # the minimum confidence\n",
    "        if confidence > 0.7:\n",
    "            # compute the (x, y)-coordinates of the bounding box for the object\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype('int')\n",
    "            \n",
    "            # ensure the bounding boxex fall within the dimension of the frame\n",
    "            (startX, startY) = (max(0, startX), max(0, startY))\n",
    "            (endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
    "            \n",
    "            # extract the face ROI and preprocess and predict\n",
    "            face = frame[startY:endY, startX:endX]\n",
    "            faces.append(face)\n",
    "            locs.append((startX, startY, endX, endY))\n",
    "            \n",
    "    # only make a predictions if atleast one face was detected\n",
    "    if len(faces) > 0:\n",
    "        # for faster inference we'll make batch predictions on all the\n",
    "        # faces at the same time rather than one-by-one prediction\n",
    "        # in the above for loop\n",
    "        for face in faces:\n",
    "            pred = with_or_without_mask(face, maskNet)\n",
    "            preds.append(pred)\n",
    "    return (locs, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading face detector model...\n"
     ]
    }
   ],
   "source": [
    "# loading our serialized face detector model from disk\n",
    "print(\"Loading face detector model...\")\n",
    "prototxtPath = 'face_detector/deploy.prototxt'\n",
    "weightsPath = 'face_detector/res10_300x300_ssd_iter_140000.caffemodel'\n",
    "\n",
    "faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
    "\n",
    "# load the face mask detector model from disk\n",
    "model.load_state_dict(torch.load('model-1-acc-99.pt'))\n",
    "\n",
    "# Starting video stream\n",
    "# vs = cv2.VideoCapture('http://192.168.225.24:8080/video')\n",
    "vs = cv2.VideoCapture(0)\n",
    "# time.sleep(2.0)\n",
    "\n",
    "# loop over the frames from the video stream\n",
    "while True:\n",
    "    \n",
    "    # grab the frame from the threaded video stram and resize to have a\n",
    "    # minimum of 400 px\n",
    "    _, frame = vs.read()\n",
    "#     frame = imutils.resize(frame, width=400)\n",
    "    \n",
    "    # detect faces in the frame and determine if they are wearing\n",
    "    # a face mask or not\n",
    "    (locs, preds) = detect_and_predict(frame, faceNet, model)\n",
    "    \n",
    "    # loop over the detected face locations and their corresponding locations\n",
    "    for (box, pred) in zip(locs, preds):\n",
    "        (startX, startY, endX, endY) = box\n",
    "        (prob, clas) = pred\n",
    "        \n",
    "        # determine the class label and color we'll use to draw\n",
    "        # the bounding box and text\n",
    "        label = \"Mask\" if clas == 0 else \"No Mask\"\n",
    "        color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
    "        \n",
    "        # include the probability in the label\n",
    "        label = \"{}: {:.2f}%\".format(label, prob * 100)\n",
    "        \n",
    "        # display the label and bounding box rectangle on the original frame\n",
    "        cv2.putText(frame, label, (startX, startY - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "        cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
    "    \n",
    "    # show the output frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    # if the `q` key is pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
